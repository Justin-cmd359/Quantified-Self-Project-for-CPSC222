{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e53f96e2",
   "metadata": {},
   "source": [
    "# Sleep Data Analysis\n",
    "Justin Yi  \n",
    "4/22/25  \n",
    "CPSC 222\n",
    "Spring 2025\n",
    "Description:  \n",
    "This Jupyter Notebook explores, interprets, and learns my sleep data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "77606685",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'utils' from 'd:\\\\School\\\\Spring 2025\\\\Intro to Data Science\\\\Data Assignments\\\\Project\\\\utils.py'>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import utils\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy.stats as stats\n",
    "import matplotlib.pyplot as plt\n",
    "import importlib \n",
    "\n",
    "importlib.reload(utils)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7053f59d",
   "metadata": {},
   "source": [
    "## Overview\n",
    "The two datasets we'll be combining and analyzing are my personal sleep data from my time here at GU, collected by Fitbit, and weather data of Spokane courtesty of MeteoStat API. This Notebook will walk you through my cleaning, EDA, and statistical analysis. Let's take a look at what we have:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e363d26e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   sleep_log_entry_id             timestamp  overall_score  composition_score  \\\n",
      "0         48988871406  2025-04-16T06:58:30Z             76                NaN   \n",
      "1         48977379538  2025-04-15T08:18:30Z             71                NaN   \n",
      "2         48967692815  2025-04-14T09:27:30Z             84                NaN   \n",
      "3         48959170544  2025-04-13T07:16:00Z             73                NaN   \n",
      "4         48948016693  2025-04-12T05:12:30Z             70                NaN   \n",
      "\n",
      "   revitalization_score  duration_score  deep_sleep_in_minutes  \\\n",
      "0                    76             NaN                   55.0   \n",
      "1                    71             NaN                   59.0   \n",
      "2                    84             NaN                  122.0   \n",
      "3                    73             NaN                   79.0   \n",
      "4                    70             NaN                   14.0   \n",
      "\n",
      "   resting_heart_rate  restlessness  \n",
      "0                  64      0.083240  \n",
      "1                  65      0.093929  \n",
      "2                  65      0.105641  \n",
      "3                  65      0.063325  \n",
      "4                  63      0.043053  \n",
      "                  date  tavg  tmin  tmax   prcp  snow   wdir  wspd  wpgt  \\\n",
      "0  2023-08-26 00:00:00  71.1  53.1  91.9  0.000  None   27.0   3.0  None   \n",
      "1  2023-08-27 00:00:00  77.2  63.0  95.0  0.000  None    8.0   2.5  None   \n",
      "2  2023-08-28 00:00:00  76.5  57.9  96.1  0.000  None  356.0   1.9  None   \n",
      "3  2023-08-29 00:00:00  66.9  60.1  75.9  0.110  None   13.0   4.5  None   \n",
      "4  2023-08-30 00:00:00  58.5  55.9  61.0  0.591  None  229.0   7.6  None   \n",
      "\n",
      "     pres  tsun  \n",
      "0  1017.7  None  \n",
      "1  1016.9  None  \n",
      "2  1015.4  None  \n",
      "3  1011.6  None  \n",
      "4  1014.2  None  \n"
     ]
    }
   ],
   "source": [
    "sleep_df = pd.read_csv(\"sleep_score.csv\")\n",
    "weather_df = utils.get_spokane_weather_df()\n",
    "weather_df.to_csv(\"initial_spokane_weather.csv\")\n",
    "print(sleep_df.head())\n",
    "print(weather_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c70adaa7",
   "metadata": {},
   "source": [
    "## Cleaning\n",
    "I'd like to join both DataFrames on their date columns to get a bigger, combined dataset, but there's some cleaning we need to do:\n",
    "1. For sleep_df:\n",
    "    * We can drop the sleep_log_entry_id column because they serve no predictive purpose and the dates already serve as unique keys\n",
    "    * We should drop instances that occur before 2023-08-26 (my first move-in day)\n",
    "    * We should remove the timestamps, keep the dates, and rename the \"timestamp\" column to \"date\"\n",
    "1. For weather_df:\n",
    "    * We can drop the \"00:00:00\" from every date because it's constant and we want consistency with sleep_df's date column\n",
    "1. We should also drop completely empty columns in both DataFrames because interpolation is literally impossible with them and they serve no purpose\n",
    "1. We should also interpolate missing values for both DataFrames if possible"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f4277d02",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\School\\Spring 2025\\Intro to Data Science\\Data Assignments\\Project\\utils.py:39: FutureWarning: DataFrame.interpolate with object dtype is deprecated and will raise in a future version. Call obj.infer_objects(copy=False) before interpolating instead.\n",
      "  df.interpolate(method=\"linear\", inplace=True)\n"
     ]
    }
   ],
   "source": [
    "# Cleaning sleep_df\n",
    "#1 Dropping ID column\n",
    "sleep_df = utils.drop_column(sleep_df, \"sleep_log_entry_id\")\n",
    "#2 Drop undesired rows\n",
    "sleep_df = utils.drop_rows_by_starting_index(sleep_df, 518, 743)\n",
    "#3 Clean timestamps\n",
    "sleep_df = utils.remove_string_ends(sleep_df, \"timestamp\", 10)\n",
    "sleep_df.rename(columns={\"timestamp\": \"date\"}, inplace=True)\n",
    "#4 Remove the empty columns\n",
    "sleep_df = utils.drop_empty_columns(sleep_df)\n",
    "#5 Address missing value on 2025-03-08; we'll interpolate\n",
    "sleep_df = utils.fill_missing_values(sleep_df)\n",
    "\n",
    "sleep_df.to_csv(\"cleaned_sleep_score.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3b84c991",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\School\\Spring 2025\\Intro to Data Science\\Data Assignments\\Project\\utils.py:39: FutureWarning: DataFrame.interpolate with object dtype is deprecated and will raise in a future version. Call obj.infer_objects(copy=False) before interpolating instead.\n",
      "  df.interpolate(method=\"linear\", inplace=True)\n"
     ]
    }
   ],
   "source": [
    "# Cleaning weather \n",
    "#1 Clean the date column\n",
    "weather_df = utils.remove_string_ends(weather_df, \"date\", 10)\n",
    "#2 Remove the empty columns\n",
    "weather_df = utils.drop_empty_columns(weather_df)\n",
    "#3 Fill empty data\n",
    "weather_df = utils.fill_missing_values(weather_df)\n",
    "weather_df.to_csv(\"cleaned_weather_test.csv\")\n",
    "\n",
    "weather_df.to_csv(\"cleaned_spokane_weather.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8097d93f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Joining both datasets\n",
    "merged_df = sleep_df.merge(weather_df, on=[\"date\"], how=\"inner\")\n",
    "merged_df.set_index(\"date\", inplace=True)\n",
    "merged_df.to_csv(\"merged_df.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff4be189",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Exploratory Data Analysis\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
